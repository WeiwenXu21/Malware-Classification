{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import numpy as np\n",
    "import string\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "def Ngram_opcode(N, opcodes_rdd):\n",
    "    '''\n",
    "        Extract and count N gram\n",
    "        '''\n",
    "    opcodes_rdd = opcodes_rdd.groupByKey().map(lambda x: (x[0],list(x[1])))\n",
    "    df = spark.createDataFrame(opcodes_rdd).toDF(\"file_names\", \"opcodes\")\n",
    "    ngram = NGram(n=N, inputCol=\"opcodes\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(df)\n",
    "    nopcode_rdd = ngramDataFrame.rdd.map(tuple).map(lambda x: (x[0],x[2])).flatMapValues(lambda x: x)\n",
    "    nocode_rdd_count = nopcode_rdd.map(lambda x: ((x),1)).reduceByKey(add)\n",
    "    return nocode_rdd_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RF(features_count_rdd,label_filename_pair):\n",
    "    '''\n",
    "        Random Forest for ranking features\n",
    "        '''\n",
    "    #---Prepare for data structure: (file_name, label, [feature1_count,feature2_count, ...])------\n",
    "    f_c = features_count_rdd\n",
    "    distinct_feature = f_c.map(lambda x: x[0][1]).distinct().sortBy(lambda x: x)\n",
    "\n",
    "    train_file_name = features_count_rdd.map(lambda x: x[0][0]).distinct().collect()\n",
    "\n",
    "    feature_filename = distinct_feature.map(lambda x: (x,train_file_name)).flatMapValues(lambda x:x)\n",
    "    feature_filename_zero = feature_filename.map(lambda x: ((x[1],x[0]),0))\n",
    "\n",
    "    full_feature_no_label = features_count_rdd.union(feature_filename_zero).reduceByKey(add)\n",
    "    full_feature_no_label = full_feature_no_label.map(lambda x: (x[0][0],(x[0][1],x[1])))\n",
    "\n",
    "    full_feature_nofilename = full_feature_no_label.sortBy(lambda x:x[1][0])\n",
    "    ordered_features = full_feature_no_label.map(lambda x:x[1][0])\n",
    "\n",
    "    full_feature_wl = full_feature_nofilename.map(lambda x: (x[0],x[1][1])).groupByKey()\n",
    "\n",
    "    full_feature_wl = label_filename_pair.join(full_feature_wl).map(lambda x: (x[0],x[1][0],Vectors.dense(list(x[1][1]))))\n",
    "    \n",
    "    \n",
    "    #---Random Forest-------------------------------------\n",
    "    df = spark.createDataFrame(full_feature_wl).toDF(\"name\",\"label\", \"features\")\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCol=\"name\", outputCol=\"indexed\")\n",
    "    si_model = stringIndexer.fit(df)\n",
    "    td = si_model.transform(df)\n",
    "\n",
    "    rf = RandomForestClassifier(numTrees=6, maxDepth=5, labelCol=\"indexed\")\n",
    "    model = rf.fit(td)\n",
    "    \n",
    "    #-------Added by Ailing, get the rdd with selected feature--------\n",
    "    feature_importance = model.featureImportances\n",
    "    \n",
    "    full_feature_rf = full_feature_wl.map(lambda x: (x[0],x[1],[x[2][i] for i in feature_importance.indices]))\n",
    "    return full_feature_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "training_data = \"data/train\"\n",
    "training_label = \"data/y_small_train.txt\"\n",
    "    \n",
    "# Read in the data\n",
    "raw_rdd_train_name_asm_data = sc.wholeTextFiles(training_data)\n",
    "rdd_label = sc.textFile(training_label)\n",
    "\n",
    "#print(raw_rdd_train_file_data.first())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name_pattern = re.compile(r'([a-zA-Z0-9]+)\\.asm')\n",
    "file_data_rdd = raw_rdd_train_name_asm_data.map(lambda x:(file_name_pattern.findall(x[0]),x[1])).map(lambda x: (x[0][0],x[1]))\n",
    "    \n",
    "\n",
    "rdd_train_name = file_data_rdd.map(lambda x: x[0])\n",
    "rdd_train_name_id = rdd_train_name.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "rdd_label_id = rdd_label.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "label_filename_pair = rdd_train_name_id.join(rdd_label_id).map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('DvdM5Zpx96qKuN3cAt1y', 'rcl'), 10), (('2F6ZfVCQRi3vrwcj4zxL', 'fmul'), 1), (('5QpgRV2cqU9wvjBist1a', 'test'), 2), (('2F6ZfVCQRi3vrwcj4zxL', 'dd'), 170), (('DvdM5Zpx96qKuN3cAt1y', 'cli'), 13), (('DvdM5Zpx96qKuN3cAt1y', 'fcmovnb'), 2), (('DvdM5Zpx96qKuN3cAt1y', 'stosb'), 20), (('2F6ZfVCQRi3vrwcj4zxL', 'jl'), 11), (('DvdM5Zpx96qKuN3cAt1y', 'repe'), 1), (('DvdM5Zpx96qKuN3cAt1y', 'clc'), 11)]\n"
     ]
    }
   ],
   "source": [
    " #---Extract opcodes--------------------------\n",
    "opcode_pattern = re.compile(r'([\\s])([A-F0-9]{2})([\\s]+)([a-z]+)([\\s+])')\n",
    "opcodes_rdd = file_data_rdd.map(lambda x: (x[0],opcode_pattern.findall(x[1]))).flatMapValues(lambda x:x).map(lambda x: (x[0],x[1][3]))\n",
    "\n",
    "#---Ngram opcode counts----------------------\n",
    "Ngram_opcode_list = []\n",
    "for i in range(4):\n",
    "    Ngram_opcode_list.append(Ngram_opcode(i+1, opcodes_rdd))\n",
    "Ngram_opcode_count = sc.union(Ngram_opcode_list)\n",
    "print(Ngram_opcode_count.collect()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('2F6ZfVCQRi3vrwcj4zxL', 'data'), 150), (('2F6ZfVCQRi3vrwcj4zxL', 'reloc'), 3), (('5QpgRV2cqU9wvjBist1a', 'HEADER'), 17), (('2F6ZfVCQRi3vrwcj4zxL', 'rsrc'), 3), (('2F6ZfVCQRi3vrwcj4zxL', 'ds'), 19), (('DvdM5Zpx96qKuN3cAt1y', 'HEADER'), 17), (('5QpgRV2cqU9wvjBist1a', 'idata'), 180), (('2F6ZfVCQRi3vrwcj4zxL', 'text'), 6104), (('DvdM5Zpx96qKuN3cAt1y', 'idata'), 253), (('2F6ZfVCQRi3vrwcj4zxL', 'rdata'), 790), (('DvdM5Zpx96qKuN3cAt1y', 'BSS'), 17), (('DvdM5Zpx96qKuN3cAt1y', 'DATA'), 1553), (('DvdM5Zpx96qKuN3cAt1y', 'reloc'), 3), (('2F6ZfVCQRi3vrwcj4zxL', 'idata'), 294), (('2F6ZfVCQRi3vrwcj4zxL', 'HEADER'), 17), (('5QpgRV2cqU9wvjBist1a', 'data'), 1020), (('DvdM5Zpx96qKuN3cAt1y', 'ds'), 1), (('DvdM5Zpx96qKuN3cAt1y', 'rsrc'), 3), (('5QpgRV2cqU9wvjBist1a', 'rsrc'), 3), (('5QpgRV2cqU9wvjBist1a', 'text'), 1356), (('5QpgRV2cqU9wvjBist1a', 'rdata'), 217), (('DvdM5Zpx96qKuN3cAt1y', 'CODE'), 77563)]\n"
     ]
    }
   ],
   "source": [
    "# ----Segment Count extraction--------------------\n",
    "segment_pattern = re.compile(r'([a-zA-Z]+):[a-zA-Z0-9]{8}[\\t\\s]')\n",
    "segment_rdd = file_data_rdd.map(lambda x: (x[0],segment_pattern.findall(x[1]))).flatMapValues(lambda x:x)\n",
    "segment_rdd_count = segment_rdd.map(lambda x: ((x),1)).reduceByKey(add)\n",
    "print(segment_rdd_count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DvdM5Zpx96qKuN3cAt1y', '6', DenseVector([17.0, 77563.0, 1553.0, 17.0, 0.0, 1.0, 253.0, 0.0, 3.0, 3.0, 0.0]))]\n",
      "(11,[0,4,6,7],[0.160714285714,0.1,0.251785714286,0.4875])\n"
     ]
    }
   ],
   "source": [
    "opcode_RF = RF(segment_rdd_count,label_filename_pair)\n",
    "print(opcode_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opcode_RF.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_full_feature_rdd(features_count_rdd):\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    # Get all the distinct features\n",
    "    distinct_feature = features_count_rdd.map(lambda x: x[0][1]).distinct().sortBy(lambda x: x)\n",
    "    # Zip feature with index format: (feature_name, index_num)\n",
    "    distinct_feature_index = distinct_feature.zipWithIndex()\n",
    "    #    print(distinct_feature.collect())\n",
    "    \n",
    "    # a vector of names for the training files\n",
    "    train_name = rdd_train_name.collect()\n",
    "    # zip the features with file name, format (feature_name,file_name)\n",
    "    feature_filename = distinct_feature.map(lambda x: (x,train_name)).flatMapValues(lambda x:x)\n",
    "    # zip feature filename pair with zeros, format ((file_name,feature_name),0)\n",
    "    feature_filename_zero = feature_filename.map(lambda x: ((x[1],x[0]),0))\n",
    "    \n",
    "    # union the two rdd. if a file does not have a count value, the value is set to zero.\n",
    "    # reformat the rdd as(file_name,(feature_name,count))\n",
    "    full_feature_no_label = features_count_rdd.union(feature_filename_zero).reduceByKey(add)\n",
    "    full_feature_no_label = full_feature_no_label.map(lambda x: (x[0][0],(x[0][1],x[1])))    \n",
    "\n",
    "    # zip with label class, format: (file_name,(label,(feature_name,count)))\n",
    "    full_feature = label_filename_pair.join(full_feature_no_label)\n",
    "    # exclude file name and sort by label\n",
    "    full_feature_nofilename = full_feature.map(lambda x: (x[1][0],x[1][1])).sortBy(lambda x:x[1][0])\n",
    "    # create dense vector for the features\n",
    "    full_feature_wl = full_feature_nofilename.map(lambda x: (x[0],x[1][1])).groupByKey().map(lambda x:(x[0],Vectors.dense(list(x[1]))))\n",
    "\n",
    "    return full_feature_wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3',\n",
       " DenseVector([17.0, 77617.0, 1553.0, 24.0, 0.0, 80.0, 1.0, 1.0, 2.0, 253.0, 0.0, 3.0, 3.0, 0.0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = spark.createDataFrame(full_feature_wl).toDF(\"label\", \"features\")\n",
    "create_full_feature_rdd(segment_rdd_count).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the rdds for ngram opcount and segment\n",
    "all_features_count = segment_rdd_count.union(Ngram_opcode_count)\n",
    "feature_full = create_full_feature_rdd(all_features_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12797"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_full.first()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(14, 4, [-0.0002, -0.9998, -0.02, -0.0003, -0.0, -0.001, -0.0, -0.0, ..., 0.0, 0.0, 0.0, -0.0003, 0.0029, -0.0, -0.0001, -0.0004], 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pca(rdd):\n",
    "    feature_vector_rdd = rdd.map(lambda x: x[1])\n",
    "    matrix = RowMatrix(feature_vector_rdd)\n",
    "    # compute the 1000 components\n",
    "    svd = matrix.computeSVD(4, computeU=True)\n",
    "    V = svd.V\n",
    "    projected = matrix.multiply(V)\n",
    "    \n",
    "    return V,projected.rows\n",
    "\n",
    "result = pca(df_full)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(14, 4, [-0.0002, -0.9986, -0.02, -0.0, 0.0075, -0.0005, -0.0, -0.0, ..., -0.0002, -0.0002, -0.0004, 0.057, 0.2114, 0.0018, -0.0001, -0.193], 0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pca1(rdd):\n",
    "    feature_vector_rdd = rdd.map(lambda x: x[1])\n",
    "    mat = RowMatrix(feature_vector_rdd)\n",
    "    pc = mat.computePrincipalComponents(4)\n",
    "\n",
    "    # Project the rows to the linear space spanned by the top 4 principal components.\n",
    "    projected = mat.multiply(pc)\n",
    "    \n",
    "    return pc,projected.rows\n",
    "result = pca1(df_full)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([17.0, 77617.0, 1553.0, 24.0, 0.0, 80.0, 1.0, 1.0, 2.0, 253.0, 0.0, 3.0, 3.0, 0.0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.map(lambda x:x[1]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
